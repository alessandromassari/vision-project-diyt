{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3209332,"sourceType":"datasetVersion","datasetId":1946896},{"sourceId":12079046,"sourceType":"datasetVersion","datasetId":7603767}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DiYT: Do it Yourself Transformer\n\n### A visual transformer by Alessandro Massari and Matteo Pelliccione developed for Vision & Perceptron exam 2024-2025 - MSc AI and Robotics - Sapienza UniversitÃ  di Roma\n\n\nComplete description of the project could be find at the following GitHub: \nhttps://github.com/alessandromassari/vision-project-diyt","metadata":{}},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/\n%rm  -r /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install packages\n!pip install pytorch-msssim \n!pip install thop\n\n# IMPORT libraries\nimport kagglehub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport random\nimport sys\nimport shutil\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom concurrent.futures import ThreadPoolExecutor as ThPE\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn import Parameter\nfrom torch import ones, zeros, tanh\nfrom einops import repeat\nimport math\nfrom pytorch_msssim import ssim\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve, precision_recall_curve\nimport seaborn as sns\nfrom thop import profile","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available and return the device\ndef hardware_check(): \n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(f\"GPU is available!\")\n        print(f\"  -> GPU - {torch.cuda.get_device_name()}\")\n        print(f\"  -> Total Memory: {torch.cuda.get_device_properties().total_memory / 1024**3:.2f} GB\")\n    else:\n        device = torch.device(\"cpu\")\n        print(\"GPU is not available, using CPU.\")\n        print(\"\\nCPU Information:\")\n\n        # Fix the backslash issue in the command string by escaping it\n        cpu_model = os.popen(\"cat /proc/cpuinfo | grep \\\"model name\\\" | uniq\").read().strip()\n        print(f\"CPU Model: {cpu_model}\")\n        print(f\"Number of CPU cores: {os.cpu_count()}\")\n\n    return device\n\ndevice = hardware_check()\nprint(f\"\\nUsing {device} for computation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utils\n\nHere all the utility functions used in this project. Rember to run them all.\n\n* Data augmentation function\n* Show dataset instances \n* Reconstruction error image plotting and saving if requested\n* Get Ground truth mask function\n* Plotter training loss\n* AUPRO custom implementation\n* plot_metric_curve","metadata":{}},{"cell_type":"code","source":"# Data Augmentation utility function\n# this fun will apply differents transformations according to the class label\n# Some transformations are specific for certain classes\n\ndef augment_image(source_image_path, dest_path, class_name, n_aug=1):\n\n  image = Image.open(source_image_path)\n  base_name = os.path.splitext(os.path.basename(source_image_path))[0]\n\n  # all class safe transformations\n  crop_resize = transforms.Compose([transforms.CenterCrop(size=200),\n            transforms.Resize((256,256))])\n  transf = [transforms.RandomRotation(degrees=30),\n            transforms.RandomPerspective(distortion_scale=0.2, p=0.8),\n            crop_resize\n  ]\n  # specific classes transformations\n  if class_name in ['carpet', 'grid', 'leather', 'wood', 'screw', 'tile']:\n    transf.append(transforms.RandomHorizontalFlip(p=0.9))\n    \n  for i in range(n_aug):\n    for j, t in enumerate(transf):\n      aug_image = t(image)\n      aug_image_name = f\"{base_name}_aug_{i}_{j}.png\"\n      aug_image.save(os.path.join(dest_path, aug_image_name))\n\n# ------------------------------------------------------------------------------------ #\n\n# Show one instance from each class in a grid\ndef show_one_instance_per_class_grid(ds_path, folder='train', instance_type='good', grid_size=(3, 5), index_in_class=0):\n\n    # Get a list of samples folders (classes)\n    samples_folders = sorted([f for f in os.listdir(ds_path) if os.path.isdir(os.path.join(ds_path, f))])\n\n    num_rows, num_cols = grid_size\n    num_subplots = num_rows * num_cols\n\n    # Limit the number of classes to display based on the grid size\n    classes_to_display = samples_folders[:num_subplots]\n\n    if not classes_to_display:\n        print(f\"No class folders found in {ds_path}\")\n        return\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))\n    # Flatten axes array for easy iteration\n    axes = axes.flatten()\n\n    fig.suptitle(f\"One Sample per Class (from {folder}/{instance_type})\", fontsize=14)\n\n    displayed_count = 0\n    for i, class_name in enumerate(classes_to_display):\n        if displayed_count >= num_subplots:\n            break # Stop if the grid is full\n\n        # Construct the path to the desired folder for the current class\n        if folder == 'train' and instance_type == 'good':\n             source_folder = os.path.join(ds_path, class_name, folder, instance_type)\n        elif folder == 'test' or folder == 'ground_truth':\n            # For test/ground_truth, instance_type is usually the defect name or 'good'\n            source_folder = os.path.join(ds_path, class_name, folder, instance_type)\n        else:\n             print(f\"Warning: Unsupported folder '{folder}' and instance_type '{instance_type}' combination for path construction.\")\n             axes[i].set_title(f\"{class_name}\\n(Path Error)\")\n             axes[i].axis(\"off\")\n             displayed_count += 1\n             continue\n\n\n        if not os.path.exists(source_folder):\n            print(f\"Folder not found for class {class_name}: {source_folder}\")\n            axes[i].set_title(f\"{class_name}\\n(Folder Not Found)\")\n            axes[i].axis(\"off\")\n            displayed_count += 1\n            continue\n\n\n        image_files = sorted([f for f in os.listdir(source_folder) if f.lower().endswith(('.bmp', '.png', '.jpg', '.jpeg'))])\n\n        if not image_files:\n            print(f\"No image files found in {source_folder}\")\n            axes[i].set_title(f\"{class_name}\\n(No Images)\")\n            axes[i].axis(\"off\")\n            displayed_count += 1\n            continue\n\n        if not 0 <= index_in_class < len(image_files):\n            print(f\"Warning: Index {index_in_class} out of bounds for class {class_name}. Using index 0.\")\n            img_to_display_path = os.path.join(source_folder, image_files[0])\n        else:\n            img_to_display_path = os.path.join(source_folder, image_files[index_in_class])\n\n        try:\n            image = Image.open(img_to_display_path).convert(\"RGB\") # Ensure RGB format\n            resized_image = image.resize((224, 224))\n            axes[i].imshow(resized_image)\n            axes[i].set_title(f\"{class_name}\\n({os.path.basename(img_to_display_path)})\", fontsize=10)\n            axes[i].axis(\"off\")\n            displayed_count += 1\n        except Exception as e:\n            print(f\"Error loading or processing image for class {class_name} ({img_to_display_path}): {e}\")\n            axes[i].set_title(f\"{class_name}\\n(Load Error)\")\n            axes[i].axis(\"off\")\n            displayed_count += 1 \n\n\n    # Hide any unused subplots if there are fewer classes than grid spots\n    for j in range(displayed_count, len(axes)):\n        axes[j].axis(\"off\")\n\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n    plt.show()\n\n# ------------------------------------------------------------------------------------ #\n\n# Utility function to plot reconstruction error in different classes \n# Visualize real, heatmap pixel-wise with ground truth mask and save picture if asve_dir specified\n\ndef visualize_with_gt(model,\n                      dataloaders,\n                      class_name,\n                      device,\n                      hyperparams,\n                      root_folder,    # es. './mvtec_ad_validation' o './mvtec_ad_test'\n                      num_images=3,\n                      save_dir=None):   # pass save_dir as arg to save the image\n\n    model.eval()\n    val_loader = dataloaders[class_name][\"val\"]\n    p = hyperparams['patch_size']\n    H = W = hyperparams['image_size']\n\n    with torch.no_grad():\n        for batch_idx, (inputs, paths) in enumerate(val_loader):\n            inputs = inputs.to(device)\n            pred, _ = model(inputs)\n\n            # Ricostruzione full-image\n            B, N, _ = pred.shape\n            npd = H // p\n            recon = (pred.view(B, npd, npd, p, p, -1)\n                         .permute(0,5,1,3,2,4)\n                         .contiguous()\n                         .view(B, -1, H, W))\n\n            # Error map pixel-wise\n            error_maps = ((recon - inputs)**2).mean(dim=1).cpu().numpy()\n\n            for i in range(min(num_images, B)):\n                img = inputs[i].cpu().permute(1,2,0).numpy()\n                err = error_maps[i]\n\n                # === calcolo percorso ground-truth ===\n                # e.g. './mvtec_ad_validation/cable/poke_insulation/007.png'\n                path = paths[i]\n                fname = os.path.basename(path)\n                parts = path.split('/')\n                # posizionati su .../<root_folder>/<class_name>/<defect_type>/<file>.png\n                # trova indice di class_name\n                \n                is_good_sample = 'good' in parts\n                gt_mask = None # Initialize ground truth mask as None\n\n                if not is_good_sample:\n                    # Try to construct and load ground truth mask path for non-'good' samples\n                    try:\n                        # Find the class_name index to get defect type\n                        class_idx = parts.index(class_name)\n                        # The folder after class_name in the path is usually the defect type\n                        defect = parts[class_idx + 1]\n                        base_fname = os.path.splitext(parts[-1])[0]\n                        # Construct the potential mask path\n                        gt_path = os.path.join(\n                            root_folder, # Use the passed root folder\n                            class_name,\n                            'ground_truth', # Ground truth masks are in this folder\n                            defect,       # Under the defect type folder\n                            f\"{base_fname}_mask.png\" # Mask filename convention\n                        )\n\n                        if os.path.exists(gt_path):\n                            gt_mask = Image.open(gt_path)\n\n                    except (ValueError, IndexError, FileNotFoundError) as e:\n                        # Handle cases where path structure is unexpected or mask not found\n                        print(f\"Could not load ground truth mask for {path}: {e}\")\n                        gt_mask = None # Ensure gt_mask is None if loading fails\n                    except Exception as e:\n                         print(f\"An unexpected error occurred loading mask for {path}: {e}\")\n                         gt_mask = None\n\n\n                #  Visualizzazione \n                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n                # Use defect type or 'good' in the title\n                display_type = defect if not is_good_sample else 'good'\n                fig.suptitle(f\"Class: {class_name} - Type: {display_type} - Image: {fname}\", fontsize=14, weight='bold')\n\n                # Original Image\n                axes[0].imshow(img)\n                axes[0].set_title(\"Original Image\")\n                axes[0].axis(\"off\")\n                \n                # Error Map\n                im1 = axes[1].imshow(err, cmap='viridis') # Use a suitable colormap for error maps\n                axes[1].set_title(\"Error Map\")\n                axes[1].axis(\"off\")\n                fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n\n                # Ground Truth Mask or Placeholder\n                if gt_mask is not None: \n                     im2 = axes[2].imshow(gt_mask, cmap='gray') # Use grayscale for masks\n                     axes[2].set_title(\"Ground Truth Mask\")\n                     axes[2].axis(\"off\")\n                else:\n                     # Display a black image if no ground truth mask (for 'good' samples or missing masks)\n                     axes[2].imshow(np.zeros((H, W), dtype=np.uint8), cmap='gray')\n                     axes[2].set_title(\"No Ground Truth Mask\")\n                     axes[2].axis(\"off\")\n\n                plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n                plt.show()\n\n                # save all the showed images in the directory passes as arg\n                if save_dir:\n                    os.makedirs(save_dir, exist_ok=True)\n                    fig.savefig(os.path.join(save_dir, f\"{class_name}_sample_{batch_idx}_{i}.png\"))\n\n# ------------------------------------------------------------------------------------ #\n\ndef get_gt_mask_path(img_path, root_folder=\"./mvtec_ad_test\"):\n    # Ensure root_folder is used correctly\n    base_fname = os.path.splitext(os.path.basename(img_path))[0]\n\n    # Extract class_name and defect type from the image path\n    # The path structure is expected to be like:\n    # <root_folder>/<class_name>/<defect_type>/<image_file>\n    parts = img_path.split('/')\n    # Find the index of the root_folder in the path\n    try:\n        root_idx = parts.index(os.path.basename(root_folder))\n    except ValueError:\n        print(f\"Error: Root folder '{os.path.basename(root_folder)}' not found in image path '{img_path}'\")\n        return None # Cannot construct the path if root is not found\n\n    # class_name is expected to be one level deeper than the root_folder\n    if root_idx + 1 < len(parts):\n        class_name = parts[root_idx + 1]\n    else:\n        print(f\"Error: Cannot extract class name from path '{img_path}'\")\n        return None\n\n    # defect type is expected to be two levels deeper than the root_folder\n    if root_idx + 2 < len(parts):\n         defect = parts[root_idx + 2]\n    else:\n         print(f\"Error: Cannot extract defect type from path '{img_path}'\")\n         return None\n\n\n    # Construct the ground truth path\n    # The ground truth masks are in <root_folder>/<class_name>/ground_truth/<defect_type>/<base_fname>_mask.png\n    # The root folder for GT masks should be the same as the input root_folder\n    gt_path = os.path.join(\n        root_folder,\n        class_name,\n        \"ground_truth\",\n        defect,\n        f\"{base_fname}_mask.png\"\n    )\n    return gt_path\n\n# ------------------------------------------------------------------------------------ #\n\n# Plot losses in training\n# Use as: plot_training_losses(training_losses) pass a dictionary as argument\n\ndef plot_training_losses(training_losses):\n    plt.figure(figsize=(10, 6))\n\n    # Palette Pantone personalizzata\n    colors = [\n        \"#822433\",\n        \"#006778\",  # PMS 3155 EC\n        \"#70A489\",  # PMS 556 EC\n        \"#00B3BE\",  # PMS 7466 EC\n        \"#AAC9B6\",  # PMS 558 EC\n        \"#AAA38E\",  # PMS 7536 EC\n        \"#D7D3C7\",  # PMS 7534 EC\n        \"#C54C00\",  # PMS 1525 EC\n        \"#F69240\",  # PMS 715 EC\n        \"#C79900\",  # PMS 117 EC\n        \"#D7A900\",  # PMS 110 EC\n        \"#A6BCC6\",  # PMS 5435 EC\n        \"#D3DEE4\",  # PMS 642 EC\n    ]\n\n    for idx, (class_name, losses) in enumerate(training_losses.items()):\n        color = colors[idx % len(colors)]  # Ciclo se le classi superano i colori disponibili\n        plt.plot(losses, label=class_name, color=color)\n\n    plt.title(\"Training Loss per Epoch for Each Class\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# ------------------------------------------------------------------------------------ #\n\n# AUPRO custom implementation \ndef compute_aupro(pred_mask, gt_mask, max_fpr=0.3):\n    gt_mask = gt_mask.astype(np.uint8).ravel()\n    pred_mask = pred_mask.ravel()\n    precision, recall, thresholds = precision_recall_curve(gt_mask, pred_mask)\n    fpr = 1 - precision\n\n    # Clip to max_fpr\n    mask = fpr <= max_fpr\n    if mask.sum() == 0:\n        return 0.0\n\n    return np.trapz(recall[mask], fpr[mask])\n\n# ------------------------------------------------------------------------------------ #\n# used in validation for plotting and saving, if required, metric curves\ndef plot_metric_curve(\n    epochs, \n    values_dict, \n    title=\"Metric Trend During Finetuning\", \n    ylabel=\"Score\", \n    save_path=None\n):\n    \n    plt.figure(figsize=(10, 5))\n    \n    colors = ['#822433', '#006778']\n    for idx, (label, values) in enumerate(values_dict.items()):\n        color = colors[idx % len(colors)]\n        plt.plot(epochs, values, marker='o', label=label, color=color)\n\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.ylim(0.0, 1.05)\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n\n    if save_path:\n        plt.show()\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"# Define dataset path here\npath = '/kaggle/input/mvtec-ad'\n# path = kagglehub.dataset_download(\"ipythonx/mvtec-ad\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset structure ð³ð\n# Is formed by groundtruth directory containing masked pictures, \n# train dir containing only good pieces and test directories filled with images with different defects\n\ndef print_directory_tree(path, prefix=\"\", is_last=True, max_folders=4, \n                         max_test_files=4, max_files_in_subfolders=4, is_root=False):\n    # Define the connector for the current item\n    connector = \"âââ \" if is_last else \"âââ \"\n\n    # Print the current item (full path if it's the root)\n    if is_root:\n        print(path)\n    else:\n        print(prefix + connector + os.path.basename(path))\n\n    # Prefix for child items\n    new_prefix = prefix + (\"    \" if is_last else \"â   \")\n\n    if os.path.isdir(path):\n        # List all files and directories in the current folder\n        items = sorted([os.path.join(path, item) for item in os.listdir(path)])\n        dirs = [item for item in items if os.path.isdir(item)]\n        files = [item for item in items if os.path.isfile(item)]\n\n        # Special logic for folders containing \"train\" and \"ground_truth\"\n        if \"train\" in path.lower() or \"ground_truth\" in path.lower():\n            # Limit to max_folders subfolders\n            show_dirs = dirs[:max_folders]\n            if len(dirs) > max_folders:\n                show_dirs.append(\"...\")  # Placeholder for remaining folders\n\n            # For each subfolder to show\n            for i, item in enumerate(show_dirs):\n                is_last_item = i == len(show_dirs) - 1\n\n                if item == \"...\":\n                    print(new_prefix + \"âââ ...\")\n                else:\n                    # Print the subfolder\n                    folder_name = os.path.basename(item)\n                    print(new_prefix + (\"âââ \" if is_last_item else \"âââ \") + folder_name)\n\n                    folder_prefix = new_prefix + (\"    \" if is_last_item else \"â   \")\n\n                    # For the first two subfolders, show max_files_in_subfolders files\n                    if i < 2:\n                        subdir_items = sorted([os.path.join(item, subitem) for subitem in os.listdir(item)])\n                        subdir_files = [f for f in subdir_items if os.path.isfile(f)]\n\n                        # Limit the number of files to show\n                        show_files = subdir_files[:max_files_in_subfolders]\n                        if len(subdir_files) > max_files_in_subfolders:\n                            show_files.append(\"...\")\n\n                        for j, file in enumerate(show_files):\n                            is_last_file = j == len(show_files) - 1\n                            if file == \"...\":\n                                print(folder_prefix + \"âââ ...\")\n                            else:\n                                print(folder_prefix + (\"âââ \" if is_last_file else \"âââ \") + os.path.basename(file))\n\n        # Special logic for folders containing \"test\"\n        elif \"test\" in path.lower():\n            # Limit to\n            show_dirs = dirs[:max_folders]\n            if len(files) > max_folders:\n                show_dirs.append(\"...\") #PLaceholder for remaining folders\n\n            # Print all files\n            for i, item in enumerate(show_dirs):\n              is_last_item = i == len(show_dirs) - 1\n              if item == \"...\":\n                print(new_prefix + \"âââ ...\")\n              else:\n                # Print the subfolder\n                folder_name = os.path.basename(item)\n                print(new_prefix + (\"âââ \" if is_last_item else \"âââ \") + folder_name)\n                folder_prefix = new_prefix + (\"    \" if is_last_item else \"â   \")\n\n                if i < 2:\n                    # For the first two subfolders, show some files\n                    subdir_items = sorted([os.path.join(item, subitem) for subitem in os.listdir(item)])\n                    subdir_files = [f for f in subdir_items if os.path.isfile(f)]\n\n\n          # For other folders, show everything normally\n        else:\n            # Print all subfolders and files\n            all_items = dirs + files\n            for i, item in enumerate(all_items):\n                is_last_item = i == len(all_items) - 1\n                print_directory_tree(item, new_prefix, is_last_item, max_folders, max_test_files, max_files_in_subfolders)\n\n\n# Print directory tree\nprint(\"ð mvtec Anomaly Detection - Directory Tree:\\n\")\nprint_directory_tree(path, is_root=True)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show instances from original mvtec-ad dataset\nshow_one_instance_per_class_grid(path, folder='train', instance_type='good', grid_size=(3, 5), index_in_class=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### â ï¸ **Choose only one from the following two code snippet! Read the initial comments!** â ï¸","metadata":{}},{"cell_type":"code","source":"# ----- This code is really time consuming, instead of run it install the augmneted dataset version in next snippet ----- #\n\n# run once in your environment, then you should be able to use the other faster snippet\n# split dataset and augment train set\n# paralellize jobs using multiple threads\n\nclasses = os.listdir(path)\nclasses = [item for item in classes if not item.endswith('.txt')]\nprint(classes)\n\nlabel_list = []\n\nprint(f\"\")\n# define the new training set folder\ntrain_folder = './mvtec_ad_train/'\n\ndef process_image(file, source_path, destination_dir, class_name, n_aug):\n    if not file.lower().endswith(('.png', '.jpg', '.jpeg')):\n        return\n    source_file_path = os.path.join(source_path, file)\n    destination_file_path = os.path.join(destination_dir, file)\n    try:\n        shutil.copy(source_file_path, destination_file_path)\n        augment_image(source_file_path, destination_dir, class_name, n_aug=n_aug)\n    except Exception as e:\n        print(f\"Error processing {file}: {e}\")\n\nmax_workers = 8\nn_aug = 1\n# copy samples in the new training set\nfor class_name in tqdm(classes, desc = \"processing\"):\n\n    # define source directories\n    source_path = os.path.join(path, class_name, 'train', 'good')\n    # define destination directory as train_subfolder/class\n    destination_dir = os.path.join(train_folder, class_name)\n    os.makedirs(destination_dir, exist_ok=True)\n\n    files = os.listdir(source_path)\n\n    with ThPE(max_workers=max_workers) as executor:\n        for file in files:\n            executor.submit(process_image, file, source_path, destination_dir, class_name, n_aug)\n\n# print the new directory tree\nprint_directory_tree(train_folder, is_root=True)\n\n\n# ------------ Visualize a sample in both original and augmented version ------------ #\n# Use it only if you want to visulize an image and its transformed version\ndef show_augmented(folder_path, class_name, n_augmented=3):\n\n  class_path = os.path.join(folder_path, class_name)\n\n  all_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg'))]\n\n  or_files = [f for f in all_files if \"_aug_\" not in f]\n  original_img_name = or_files[8]\n  original_img_path = os.path.join(class_path, original_img_name)\n  original_img = Image.open(original_img_path)\n\n  base_original_name = os.path.splitext(original_img_name)[0]\n  aug_imgs = [f for f in all_files if f.startswith(base_original_name + \"_aug_\")]\n  aug_imgs = sorted(aug_imgs)[:n_augmented]\n\n\n  fig, axes = plt.subplots(1, 1 + len(aug_imgs), figsize=(4*(1 + len(aug_imgs)), 4))\n  fig.suptitle(f\"Original + Augmentations - Class: {class_name}\", fontsize=14, weight='bold')\n\n  axes = np.ravel(axes)\n\n  axes[0].imshow(original_img)\n  axes[0].set_title(\"Original\")\n  axes[0].axis(\"off\")\n\n  for i, aug_name in enumerate(aug_imgs):\n      aug_img = Image.open(os.path.join(class_path, aug_name))\n      axes[i + 1].imshow(aug_img)\n      axes[i + 1].set_title(f\"Augmented {i+1}\")\n      axes[i + 1].axis(\"off\")\n\n  plt.tight_layout()\n  plt.show()\n\ntrain_folder = './mvtec_ad_train/'  # o '/kaggle/working/mvtec_ad_train/'\nshow_augmented(train_folder, class_name='wood')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This is the alternative to previous code snippet! don't run if you've already ran the previous one! \n\n# Augmented dataset directly from Kaggle (is the result of previous code snippet but ran by us)\naug_trainset_folder = '/kaggle/input/mvtec-ad-augmneted-trainset/mvtec_ad_train' \nclasses = sorted(os.listdir(aug_trainset_folder))\nprint(\"Augmented trainset classes: \", classes)\n\n# print the directory tree of the augmeted dataset\nprint_directory_tree(aug_trainset_folder, is_root=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split orginal dataset and do the same for validation and test \nvalidation_folder = './mvtec_ad_validation/'\ntest_folder = './mvtec_ad_test/'\n\n#20% validation 80% test\nsplit_ratio = 0.2\n\nfor class_name in classes:\n\n  # define source directories: here we've subclasses too\n  source_path = os.path.join(path, class_name, 'test')\n  sub_folders = os.listdir(source_path)\n  for s in sub_folders:\n  \n    source_sub_path = os.path.join(source_path, s)\n    files = [f for f in os.listdir(source_sub_path)\n              if f.lower().endswith(('.png','.jpg','.bmp'))]\n    # split using sklearn train_test_split function, random_state = seed\n    test_files, val_files = train_test_split(files, test_size=split_ratio,random_state=46,shuffle=True)\n\n    # validation set\n    dst_val_sub = os.path.join(validation_folder, class_name, s)\n    os.makedirs(dst_val_sub, exist_ok=True)\n    for f in val_files:\n        shutil.copy(os.path.join(source_sub_path,f), os.path.join(dst_val_sub,f))\n\n        mask_filename = os.path.splitext(f)[0] + '_mask.png'\n        source_mask_path = os.path.join(path, class_name, 'ground_truth', s, mask_filename)\n\n        if os.path.exists(source_mask_path):\n            dst_mask_sub = os.path.join(validation_folder, class_name, 'ground_truth', s)\n            os.makedirs(dst_mask_sub, exist_ok=True)\n            shutil.copy(source_mask_path, os.path.join(dst_mask_sub, mask_filename))\n      \n    # test set\n    dst_test_sub = os.path.join(test_folder, class_name, s)\n    os.makedirs(dst_test_sub, exist_ok=True)\n    for f in test_files:\n        shutil.copy(os.path.join(source_sub_path,f), os.path.join(dst_test_sub,f))\n        \n        mask_filename = os.path.splitext(f)[0] + '_mask.png'\n        source_mask_path = os.path.join(path, class_name, 'ground_truth', s, mask_filename)\n\n        if os.path.exists(source_mask_path):\n            dst_mask_sub = os.path.join(test_folder, class_name, 'ground_truth', s)\n            os.makedirs(dst_mask_sub, exist_ok=True)\n            shutil.copy(source_mask_path, os.path.join(dst_mask_sub, mask_filename))\n\n# print directory trees - uncomment following threee lines if you wanna see them\n            \nprint_directory_tree(test_folder, is_root=True)\nprint(f\"\")\nprint_directory_tree(validation_folder, is_root=True)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Loader ...","metadata":{}},{"cell_type":"code","source":"# e.g. classes = ['bottle', 'cable']\n# return a dictionary of dataloaders with keys train, val, test \n# new class to handle validation set structure (same structure of test set)\nclass extract_images_dir(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.transform = transform\n        self.samples = []\n\n        for dirpath, _, filenames in os.walk(root_dir):\n            # exclude ground_truth from validation and test images\n            if 'ground_truth' in dirpath.split('/'):\n                continue\n                \n            for f in sorted(filenames):\n                if f.endswith((\".png\", \".jpg\", \".jpeg\")):\n                    img_path = os.path.join(dirpath, f)\n                    # double check to exclude '_mask' objects - should be a redundant check\n                    if '_mask' in os.path.splitext(img_path)[0].lower():\n                        continue\n                    self.samples.append(img_path)\n                \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, img_path\n        \ndef create_class_dataloader(classes,\n    trainset_folder = '/kaggle/input/mvtec-ad-augmneted-trainset/mvtec_ad_train',\n    validation_folder = './mvtec_ad_validation/',\n    test_folder = './mvtec_ad_test/',\n    batch_size=32):\n\n    \n    transform = transforms.Compose([\n        transforms.Resize((256,256)),\n        transforms.ToTensor()\n    ])\n\n    # we load each class separatly so a dictionary of dataloaders\n    dataloaders = {}   \n    \n    # iterate for each class c in classes\n    for c in classes:\n        \n        print(f\"Creating dataloader for class: {c}\")\n        \n        class_trainpath = os.path.join(trainset_folder, c)\n        class_validpath = os.path.join(validation_folder, c)\n        class_testpath  = os.path.join(test_folder, c)\n        \n        # training set\n        train_set =   extract_images_dir(class_trainpath, transform=transform)\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n     \n        # validation set it's a little bit more complex\n        validation_set =  extract_images_dir(class_validpath, transform=transform)\n        val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=4)\n\n        # test set \n        test_set =  extract_images_dir(class_testpath, transform=transform)\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n        \n        # update with new class the dataloaders dictionary\n        dataloaders[c] = {\n            \"train\": train_loader,\n            \"val\": val_loader,\n            \"test\": test_loader,\n        }\n    return dataloaders","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Network","metadata":{}},{"cell_type":"code","source":"# here the model paramerers\n\n#seed\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nrandom.seed(42)\n\nhyperparameters = {\n    'image_size' : 256,   # prev val: 224\n    'patch_size' :  16,   # 256/32^2 = 64 # of patches\n    'in_channels': 3,\n    'embed_dim' : 512,    # prev val: 512\n    'num_heads' : 8,      # prev val: 16 - 8\n    'depth_enc' : 16,     # prev val: 16\n    'depth_dec' : 2,      # prev val: 8 - 2\n    'mlp_dim'   : 512,    # prev val: 1024\n    'dropout_rate' : 0.1,\n    'init_alpha' : 0.5,\n    'mask_ratio' : 0.75,\n    'dec_embed_dim': 256  # prev: 512\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DyT normalization layer definition\n\nclass DyT(nn.Module):\n    def __init__(self, C, init_alpha=0.1):\n        super().__init__()\n        self.alpha = Parameter(ones(1) * init_alpha)\n        self.gamma = Parameter(ones(C))\n        self.beta = Parameter(zeros(C))\n        \n    def forward(self, x):\n        x = tanh(self.alpha * x)\n        return self.gamma * x + self.beta\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# New Feature Aggregration module \n\nclass FeatureAggregationModule(nn.Module):\n    def __init__(self, embed_dim, patch_size, image_size, intermediate_layers):\n        super().__init__()\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.intermediate_layers = intermediate_layers\n        self.embed_dim = embed_dim\n\n        # The target spatial dimensions for the aggregated features\n        self.target_h = self.target_w = image_size // patch_size\n        self.num_target_patches = self.target_h * self.target_w\n\n        # Linear layers to project features from different encoder layers to a common dimension\n        self.projection_layers = nn.ModuleList([\n            nn.Linear(embed_dim, embed_dim) for _ in intermediate_layers\n        ])\n\n        # A final layer to combine features\n        # The input channels to the combiner will be the sum of the projected feature dimensions\n        self.combiner = nn.Conv2d(len(intermediate_layers) * embed_dim, embed_dim, kernel_size=1)\n\n    # Modified forward to accept intermediate features (list of (features, ids_keep) tuples) and original N\n    def forward(self, intermediate_features_with_ids, original_N):\n        # intermediate_features_with_ids is a list of (tensor, ids_keep) tuples\n        # tensor shape is (B, len_keep, D), ids_keep shape is (B, len_keep)\n\n        batch_aggregated_features_spatial = []\n\n        for b in range(intermediate_features_with_ids[0][0].size(0)): # Iterate through batch\n            batch_resized_features = []\n            for i, (features, ids_keep) in enumerate(intermediate_features_with_ids):\n                # features: (B, len_keep_i, D), ids_keep: (B, len_keep_i)\n                features_b = features[b] # (len_keep_i, D)\n                ids_keep_b = ids_keep[b] # (len_keep_i)\n\n                # Create a tensor for the full grid (N, D) and scatter the kept features\n                full_grid_features = torch.zeros(original_N, self.embed_dim, device=features_b.device, dtype=features_b.dtype)\n                # Use a boolean mask to select valid indices in ids_keep_b (not padding value)\n                valid_indices_mask = ids_keep_b != -1 # Assuming -1 is the padding value\n\n                # Scatter the features using only valid indices\n                full_grid_features[ids_keep_b[valid_indices_mask]] = features_b[valid_indices_mask] # Scatter features to original positions\n\n\n                # Reshape full grid features from (N, D) to spatial (D, H/P, W/P)\n                # Need to ensure N is equal to self.num_target_patches\n                if original_N != self.num_target_patches:\n                     raise ValueError(f\"Original number of patches ({original_N}) does not match target grid size ({self.num_target_patches}).\")\n\n                features_spatial = full_grid_features.transpose(0, 1).view(self.embed_dim, self.target_h, self.target_w) # (D, H/P, W/P)\n\n\n                # Project features (optional) - Project after reshaping to spatial\n                features_projected = self.projection_layers[i](features_spatial.permute(1, 2, 0)).permute(2, 0, 1) # (D, H/P, W/P)\n\n                batch_resized_features.append(features_projected)\n\n            # Concatenate features along the channel dimension for the current batch item\n            combined_features_b = torch.cat(batch_resized_features, dim=0) # (len(intermediate_layers) * D, H/P, W/P)\n            batch_aggregated_features_spatial.append(combined_features_b)\n\n        # Stack the aggregated features for the batch\n        aggregated_features_spatial = torch.stack(batch_aggregated_features_spatial, dim=0) # (B, len(intermediate_layers) * D, H/P, W/P)\n\n        # Combine features using the combiner layer\n        aggregated_features = self.combiner(aggregated_features_spatial) # (B, embed_dim, H/P, W/P)\n\n        return aggregated_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Building ViT with DyT as normalizer\n\n# Building ViT\ndef random_masking(x, mask_ratio):\n    B, N, D = x.shape\n    len_keep = int(N * (1 - mask_ratio))\n\n    # Assume the patches are arranged in a square grid (e.g. sqrt(N) x sqrt(N))\n    grid_size = int(math.sqrt(N))\n    assert grid_size * grid_size == N, \"N should be a perfect square for block masking\"\n\n    x_masked_list = []\n    mask_list = []\n    ids_restore_list = []\n    ids_keep_list = []\n    ids_masked_list = []\n\n    for b in range(B):\n        # Create a 2D mask: 1 = masked, 0 = keep\n        mask_2d = torch.ones(grid_size, grid_size, device=x.device)\n\n        # Determine number of patches to unmask\n        num_keep = len_keep\n\n        while mask_2d.sum() > (N - num_keep):\n            # Randomly choose a block size\n            block_size = torch.randint(low=2, high=grid_size // 2 + 1, size=(1,)).item()\n            top = torch.randint(0, grid_size - block_size + 1, size=(1,)).item()\n            left = torch.randint(0, grid_size - block_size + 1, size=(1,)).item()\n\n            # Unmask a block (set to 0)\n            mask_2d[top:top+block_size, left:left+block_size] = 0\n\n        # Flatten mask\n        mask_flat = mask_2d.flatten()\n        ids_keep = torch.nonzero(mask_flat == 0, as_tuple=False).squeeze(1)\n        ids_masked = torch.nonzero(mask_flat == 1, as_tuple=False).squeeze(1)\n\n        # Shuffle ids to simulate \"restore\" operation\n        ids_all = torch.cat([ids_keep, ids_masked], dim=0)\n        ids_restore = torch.argsort(ids_all)\n\n        # Apply mask\n        x_b = x[b:b+1]\n        x_masked_b = torch.gather(x_b, dim=1, index=ids_keep.unsqueeze(0).unsqueeze(-1).repeat(1, 1, D))\n\n        # Create full mask\n        mask_b = torch.ones([1, N], device=x.device, dtype=torch.bool) # Use boolean mask\n        mask_b[0, ids_keep] = False # False means keep\n        mask_b = torch.gather(mask_b, dim=1, index=ids_restore.unsqueeze(0))\n\n        x_masked_list.append(x_masked_b.squeeze(0)) # Remove batch dim for padding\n        mask_list.append(mask_b.squeeze(0))\n        ids_restore_list.append(ids_restore)\n        ids_keep_list.append(ids_keep)\n        ids_masked_list.append(ids_masked)\n\n\n    # Pad tensors to the maximum length in the batch\n    max_len_masked = max(len(t) for t in x_masked_list)\n    max_len_restore = max(len(t) for t in ids_restore_list)\n    max_len_keep = max(len(t) for t in ids_keep_list)\n    max_len_masked_ids = max(len(t) for t in ids_masked_list)\n\n    x_masked_padded = torch.stack([F.pad(t, (0, 0, 0, max_len_masked - t.size(0))) for t in x_masked_list], dim=0)\n    mask_padded = torch.stack([F.pad(t, (0, max_len_restore - t.size(0)), value=True) for t in mask_list], dim=0) # Pad mask with True (masked)\n    ids_restore_padded = torch.stack([F.pad(t, (0, max_len_restore - t.size(0)), value=-1) for t in ids_restore_list], dim=0) # Pad with -1 or another indicator\n\n    # Need padded versions of ids_keep and ids_masked for the decoder reconstruction\n    ids_keep_padded = torch.stack([F.pad(t, (0, max_len_keep - t.size(0)), value=-1) for t in ids_keep_list], dim=0)\n    ids_masked_padded = torch.stack([F.pad(t, (0, max_len_masked_ids - t.size(0)), value=-1) for t in ids_masked_list], dim=0)\n\n    # Return padded tensors and the padded ids for reconstruction\n    return x_masked_padded, mask_padded, ids_restore_padded, ids_keep_padded, ids_masked_padded\n\n\n# Sinuisodal patch embedding to\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.patch_size = patch_size\n        self.image_size = image_size\n\n        # Projection layer with initialization\n        self.proj = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        num_patches = (image_size // patch_size) ** 2\n        self.num_patches = num_patches\n        self.grid_size = image_size // patch_size\n\n        # Learnable positional embeddings\n        self.learnable_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n        nn.init.trunc_normal_(self.learnable_pos_embed, std=0.02)\n\n        # Sinusoidal positional embeddings\n        self.sinusoidal_pos_embed = self._build_sinusoidal_pos_embed(num_patches, embed_dim)\n\n    def _build_sinusoidal_pos_embed(self, num_patches, embed_dim):\n        grid_size = int(math.sqrt(num_patches))\n        coords_h = torch.arange(grid_size)\n        coords_w = torch.arange(grid_size)\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n        coords_flatten = torch.flatten(coords, 1) # (2, num_patches)\n\n        pe = torch.zeros(num_patches, embed_dim)\n        position = coords_flatten.transpose(0, 1) # (num_patches, 2)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n\n        pe[:, 0::2] = torch.sin(position[:, 0:1] * div_term)\n        pe[:, 1::2] = torch.cos(position[:, 0:1] * div_term)\n        pe = pe.unsqueeze(0) # (1, num_patches, embed_dim)\n        return pe.to(self.proj.weight.device) # Move to the same device as proj weight\n\n\n    def forward(self, x: torch.Tensor):\n        B = x.size(0)\n        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n\n        # Add both learnable and sinusoidal positional embeddings\n        learnable_pe = self.learnable_pos_embed.to(x.device)\n        sinusoidal_pe = self.sinusoidal_pos_embed.expand(B, -1, -1).to(x.device)\n\n        x = x + learnable_pe + sinusoidal_pe\n\n        return x\n\nclass MLP(nn.Module):\n   def __init__(self, in_features, hidden_features, dropout_rate):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.init_weights()\n\n   def init_weights(self):\n        # Xavier initialization for better convergence\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.zeros_(self.fc1.bias)\n        nn.init.zeros_(self.fc2.bias)\n\n   def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.dropout2(x)\n        return x\n\n# Encoder\nclass EncoderLayer(nn.Module):\n  def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate):\n        super().__init__()\n        # Pre-norm architecture with DyT\n        self.norm1 = DyT(embed_dim, init_alpha=0.1)\n        self.attn = nn.MultiheadAttention(\n            embed_dim, num_heads,\n            dropout=dropout_rate,\n            batch_first=True\n        )\n        self.norm2 = DyT(embed_dim, init_alpha=0.1)\n        self.mlp = MLP(embed_dim, mlp_dim, dropout_rate)\n\n  def forward(self, x):\n        # Pre-norm\n        norm_x = self.norm1(x)\n        attn_out, _ = self.attn(norm_x, norm_x, norm_x)\n        x = x + attn_out\n\n        norm_x = self.norm2(x)\n        mlp_out = self.mlp(norm_x)\n        x = x + mlp_out\n        return x\n\n# Decoder\nclass DecoderLayer(nn.Module):\n    def __init__(self, dec_embed_dim, num_heads, mlp_dim, dropout_rate, enc_embed_dim):\n        super().__init__()\n        self.norm1 = DyT(dec_embed_dim, init_alpha=0.1)\n        self.attn = nn.MultiheadAttention(\n            dec_embed_dim, num_heads,\n            dropout=dropout_rate,\n            batch_first=True\n        )\n\n        # cross-attention\n        self.norm2 = DyT(dec_embed_dim, init_alpha=0.1)\n        # Ensure cross-attention key/value projection matches encoder embed dim\n        self.cross_attn = nn.MultiheadAttention(\n            embed_dim=dec_embed_dim, # Query dimension is decoder embed dim\n            num_heads=num_heads,\n            dropout=dropout_rate,\n            batch_first=True,\n            kdim=enc_embed_dim,  # Key dimension is encoder embed dim\n            vdim=enc_embed_dim   # Value dimension is encoder embed dim\n        )\n\n\n        # mlp layer\n        self.norm3 = DyT(dec_embed_dim, init_alpha=0.1) # Renamed to norm3\n        self.mlp = MLP(dec_embed_dim, mlp_dim, dropout_rate) # Use dec_embed_dim for MLP\n\n    def forward(self, x, encoder_output_cross_attention):\n\n        # self attention\n        norm_x = self.norm1(x)\n        attn_out, _ = self.attn(norm_x, norm_x, norm_x)\n        x = x + attn_out\n\n\n        # cross attention on encoder output\n        norm_x = self.norm2(x)\n        # Pass encoder_output_cross_attention as key and value\n        cross_attn_out, _ = self.cross_attn(norm_x, encoder_output_cross_attention, encoder_output_cross_attention)\n        x = x + cross_attn_out\n\n        # mlp layer\n        norm_x = self.norm3(x) # Use norm3\n        mlp_out = self.mlp(norm_x)\n        x = x + mlp_out\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MAE(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim,\n                 num_heads, depth_enc, depth_dec, mlp_dim, dropout_rate, init_alpha, mask_ratio, dec_embed_dim=256):\n        super().__init__()\n\n        # Patch embedding with better initialization\n        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n\n        # Store attributes used in training loop later\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.in_channels = in_channels\n\n        # Encoder with ModuleList (or Sequantial)\n        self.encoder = nn.ModuleList([\n            EncoderLayer(embed_dim, num_heads, mlp_dim, dropout_rate)\n            for _ in range(depth_enc)\n        ])\n        self.encoder_norm = DyT(embed_dim, init_alpha=init_alpha)\n\n        # Decoder-specific projection\n        self.decoder_embed = nn.Linear(embed_dim, dec_embed_dim)\n        nn.init.xavier_uniform_(self.decoder_embed.weight)\n        nn.init.zeros_(self.decoder_embed.bias)\n\n        # Mask token with proper initialization\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, dec_embed_dim))\n        nn.init.trunc_normal_(self.mask_token, std=0.02)\n\n        # Decoder\n        self.decoder = nn.ModuleList([\n            DecoderLayer(dec_embed_dim, num_heads, mlp_dim, dropout_rate, enc_embed_dim=embed_dim)\n            for _ in range(depth_dec)\n        ])\n        self.decoder_norm = DyT(dec_embed_dim, init_alpha=init_alpha)\n\n        # Prediction head with proper initialization\n        self.decoder_pred = nn.Linear(dec_embed_dim, patch_size * patch_size * in_channels)\n        nn.init.xavier_uniform_(self.decoder_pred.weight)\n        nn.init.zeros_(self.decoder_pred.bias)\n\n        # NEW PART - MAT PYRAMID\n        self.intermediate_layers = [depth_enc // 3, 2 * depth_enc // 3, depth_enc]\n        self.feature_aggregator = FeatureAggregationModule(embed_dim, patch_size, image_size, self.intermediate_layers)\n\n        aggregated_patches_h = aggregated_patches_w = self.image_size // self.patch_size\n        num_aggregated_patches = aggregated_patches_h * aggregated_patches_w\n        # final linear layer\n        self.aggregated_feature_proj = nn.Linear(embed_dim, embed_dim)\n\n        # Apply weight initialization\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n\n        if isinstance(module, nn.Linear):\n            if hasattr(module, 'weight') and module.weight is not None:\n                if module.weight.shape[0] == module.weight.shape[1]:  # Square matrix (like in attention)\n                    nn.init.xavier_uniform_(module.weight)\n                else:\n                    nn.init.xavier_normal_(module.weight)\n            if hasattr(module, 'bias') and module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.MultiheadAttention):\n            # MultiheadAttention has its own initialization, but we can fine-tune it\n            if hasattr(module, 'in_proj_weight') and module.in_proj_weight is not None:\n                nn.init.xavier_uniform_(module.in_proj_weight)\n            if hasattr(module, 'out_proj.weight'):\n                nn.init.xavier_uniform_(module.out_proj.weight)\n\n    def forward(self, x, mask_ratio=0.75):\n        x = self.patch_embed(x)\n        B, N, D = x.shape\n\n        # use masking only in training - no masking for test\n        if self.training:\n            x_masked, mask, ids_restore, ids_keep, ids_masked_original_order = random_masking(x, mask_ratio)\n        else:\n            x_masked = x\n            mask=torch.zeros(B,N, device=x.device, dtype=torch.bool)\n            ids_keep = torch.arange(N, device=x.device).unsqueeze(0).repeat(B, 1)\n            ids_restore = torch.arange(N, device=x.device).unsqueeze(0).repeat(B, 1)\n\n\n        intermediate_features = []\n        x_encoder = x_masked\n        for i, layer in enumerate(self.encoder):\n            x_encoder = layer(x_encoder)\n            if (i + 1) in self.intermediate_layers:\n                intermediate_features.append((x_encoder, ids_keep))\n\n        encoder_output_last_layer = self.encoder_norm(x_encoder)\n\n        aggregated_features_spatial = self.feature_aggregator(intermediate_features, N)\n\n        B, D_agg, H_agg, W_agg = aggregated_features_spatial.shape\n        aggregated_features_flat = aggregated_features_spatial.flatten(2).transpose(1, 2)\n        aggregated_features_projected = self.aggregated_feature_proj(aggregated_features_flat)\n\n        len_keep_actual = encoder_output_last_layer.size(1)\n\n        # Project encoder output to decoder dimension before concatenation\n        encoder_output_projected = self.decoder_embed(encoder_output_last_layer)\n\n        mask_tokens = self.mask_token.repeat(B, N - len_keep_actual, 1)\n        x_ = torch.cat([encoder_output_projected, mask_tokens], dim=1)\n\n        # expanding restoration\n        ids_restore_expanded = ids_restore.unsqueeze(-1).repeat(1, 1, self.decoder_embed.out_features) # Use decoder embed dim\n\n        # Pad x_ if its length is less than the maximum length in ids_restore_expanded\n        max_len_restore = ids_restore_expanded.size(1)\n        if x_.size(1) < max_len_restore:\n            padding_length = max_len_restore - x_.size(1)\n            x_ = F.pad(x_, (0, 0, 0, padding_length)) # Pad along the sequence dimension\n\n\n        x_ = torch.gather(x_, dim=1, index=ids_restore_expanded)\n\n\n        for layer in self.decoder:\n            x_ = layer(x_, aggregated_features_projected)\n        x_ = self.decoder_norm(x_)\n\n        pred = self.decoder_pred(x_) # shape (B, N, patch_size**2 * in_channels)\n\n        return pred, mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sanity check\n\nVerify:\n* Batch type and shapes\n* I/O shape\n* Model size and number of parameters\n* FLOPs required by the model \n\n### â ï¸ **To have a correct training procedure you MUST run this section of code, it's more than just a check.** â ï¸","metadata":{}},{"cell_type":"code","source":"def mae_sanity_check(model, dataloader, device, hyperparameters):\n    model = model.to(device)\n    model.eval()\n    print(f\"----- Model Sanity check started ----\")\n    \n    # no gradients here, its's a check not a training :)\n    with torch.no_grad():\n        for _, sample in enumerate(dataloader):\n            if isinstance(sample, (list, tuple)):\n                input_sample = sample[0]  # sample = (image, label), prendiamo l'immagine\n            else:\n                input_sample = sample  # solo immagini\n\n            input_sample = input_sample.to(device)\n\n            # === SANITY CHECK ===\n            assert input_sample.dtype == torch.float32, \"Input tensor should be float32\"\n            assert input_sample.size(1) == hyperparameters['in_channels'], f\"Expected {hyperparameters['in_channels']} input channels\"\n            print(\"Input check: OK\")\n\n            # === MODEL FORWARD ===\n            pred, mask = model(input_sample)\n\n            # pred: [B, N, patch_dim]\n            assert pred.dtype == torch.float32, \"Prediction tensor should be float32\"\n            assert pred.size(0) == input_sample.size(0), \"Batch size mismatch between input and prediction\"\n            assert pred.size(2) == (hyperparameters['patch_size'] ** 2) * hyperparameters['in_channels'], \\\n                f\"Each patch prediction should have size patch^2 * in_channels\"\n\n            print(\"Output prediction shape check: OK\")\n\n            break  # controlliamo un solo batch\n\n    # Evaluate # of parameters and size\n    total_params = sum(p.numel() for p in model.parameters())\n    size_in_mb = total_params * 4 / 1024 / 1024\n    print(f\"Model size: {size_in_mb:.2f} MB ({total_params/ 1e6:.2f} M params)\")\n    print(\"Model forward sanity check: PASSED ð¯\")\n    print(\"\")\n\n    # FLOPs evaluation\n    #dummy_input =  torch.randn(1, hyperparameters['in_channels'], hyperparameters['image_size'], hyperparameters['image_size']).to(device)\n    flops, params = profile(model, inputs=(input_sample, ))\n    print(f\"Model FLOPs: {flops / 1e9:.2f} GigaFLOPs\") # Print FLOPs in GigaFLOPs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = ['capsule','carpet','grid','hazelnut','leather']# write here the classes you want to train and test! \ndataloaders = create_class_dataloader(classes=classes,batch_size=32)\nmodel = MAE(**hyperparameters)\nval_loader_to_check = dataloaders['hazelnut']['val']\nmae_sanity_check(model, val_loader_to_check, device, hyperparameters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T21:57:42.968636Z","iopub.execute_input":"2025-06-18T21:57:42.968956Z","iopub.status.idle":"2025-06-18T21:57:49.370283Z","shell.execute_reply.started":"2025-06-18T21:57:42.968930Z","shell.execute_reply":"2025-06-18T21:57:49.369358Z"}},"outputs":[{"name":"stdout","text":"Creating dataloader for class: capsule\nCreating dataloader for class: carpet\nCreating dataloader for class: hazelnut\nCreating dataloader for class: leather\n----- Model Sanity check started ----\nInput check: OK\nOutput prediction shape check: OK\nModel size: 120.65 MB (31.63 M params)\nModel forward sanity check: PASSED ð¯\n\n[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\nModel FLOPs: 73.69 GigaFLOPs\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# from an image to patches - patchification process\ndef target_to_patches(pred, target, patch_size, in_channels):\n\n    B, N, _ = pred.shape\n    \n    patches = target.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n    patches = patches.contiguous().view(B, in_channels, -1, patch_size, patch_size)\n    patches = patches.permute(0, 2, 1, 3, 4).contiguous().view(B, N, -1)  # (B, N, P*P*C)\n    \n    return patches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DEFINE LOSS FUNCTIONS\n\n# Define the training loss function taking in account both MSE and SSIM loss\ndef mae_loss_with_ssim(pred, target, mask, patch_size, in_channels):\n    \"\"\"\n    pred: (B, N, P*P*C)\n    target: (B, C, H, W)\n    mask: (B, N)\n    \"\"\"\n    \n    B, N, _ = pred.shape\n    device = pred.device\n\n    target_patches = target_to_patches(pred, target, patch_size, in_channels)\n    \n    # PATCH-BASED MSE (su patch mascherate)\n    mse_loss = (pred - target_patches) ** 2\n\n    if mask.sum() > 0:\n        mask = mask.unsqueeze(-1).type_as(mse_loss)  # (B, N, 1)\n        mse_loss = (mse_loss * mask).sum() / (mask.sum() * mse_loss.size(-1))\n    else:\n        mse_loss = mse_loss.mean()\n\n    # === SSIM LOSS (immagine ricostruita vs originale) ===\n    # Ricostruisci immagine da pred\n    H = W = target.shape[2]\n    recon = pred.view(B, H // patch_size, W // patch_size, patch_size, patch_size, in_channels)\n    recon = recon.permute(0, 5, 1, 3, 2, 4).contiguous()\n    recon = recon.view(B, in_channels, H, W)\n\n    ssim_loss = 1 - ssim(recon, target, data_range=1.0, size_average=True)  # Assumendo immagini normalizzate [0,1]\n\n    # Total loss\n    total_loss = 0.8 * mse_loss + 0.2 * ssim_loss \n\n    return total_loss\n\n# Define pixel wise loss used in fine-tuning\ndef pixelwise_loss_mean(pred, target, patch_size, in_channels):\n    \"\"\"\n    pred: (B, N, P*P*C)\n    target: (B, C, H, W)\n    \"\"\"\n    B, N, _ = pred.shape\n    device = pred.device\n\n    H = W = target.shape[2]\n    \n    # Reconstruct images from patches\n    recon = pred.view(B, H // patch_size, W // patch_size, patch_size, patch_size, in_channels)\n    recon = recon.permute(0, 5, 1, 3, 2, 4).contiguous()\n    recon = recon.view(B, in_channels, H, W)\n\n    # Loss MSE pixel-wise\n    return F.mse_loss(recon, target)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training per class - one train step\n\n# Define training parameters\nBATCH_SIZE= 32  \nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 1e-2\n\n#dictionary for per class train loss plot\ntraining_losses = {}\n\ndef train_one_ep(model, dataloader, optimizer, criterion, c_name, device):\n    \n    model.train()\n    running_loss = 0.0\n    for batch in dataloader:\n        ins = batch[0].to(device)\n        optimizer.zero_grad()\n        pred, mask = model(ins)\n\n        # training and finetuning case: two different losses\n        if criterion == pixelwise_loss_mean:\n            loss = pixelwise_loss_mean(pred, ins, model.patch_size, model.in_channels)\n        else:    \n            loss = criterion(pred, ins, mask, model.patch_size, model.in_channels) #model.patch_embed.proj.in_channels)\n        \n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    return running_loss / len(dataloader)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\n# First pre-training, then run the fine-tuning section to get the overall results\nEPOCHS = 80\nPATIENCE_T = 20\n\nfor c_name, dt_loader in dataloaders.items():\n    print(f\"\\nRunning training on class: {c_name} \")\n    \n    model = MAE(**hyperparameters).to(device)\n    train_loader = dt_loader[\"train\"]\n    val_loader   = dt_loader[\"val\"]\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n    best_tl = 0.25  # just for the development\n    early_stop_count = 0\n    if c_name not in training_losses :\n        training_losses[c_name] = []\n    \n    # Learning Rate Scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=0.0)\n    \n    for epoch in range(EPOCHS):\n        print(f\"CLASS: {c_name} | EPOCH: {epoch+1}/{EPOCHS}\")\n        train_loss = train_one_ep(model, train_loader, optimizer, mae_loss_with_ssim, c_name, device)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        training_losses[c_name].append(train_loss)\n        scheduler.step()\n        \n        # Early stopping\n        if (train_loss < best_tl):\n            best_tl = train_loss\n            early_stop_count = 0\n            torch.save(model.state_dict(), f\"/kaggle/working/{c_name}_pretrained_model.pth\")\n            print(f\" New best model saved for class: {c_name}\")\n        else:\n            early_stop_count += 1\n            if early_stop_count >= PATIENCE_T:\n                print(\"Early stopping triggered.\")\n                break\n   \n    \n    print(f\"-- Finished training for class: {c_name} --\\n\")\n\nprint(\"\\n---- Training complete for all classes ----\\n\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot losses-epochs graph\nplot_training_losses(training_losses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Change class_name to visualize different class samples\nvisualize_with_gt(model, dataloaders, class_name=\"hazelnut\", device=device, \n                  hyperparams=hyperparameters,root_folder='./mvtec_ad_validation', num_images=4, save_dir='./pictures/pretrained')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evalutation function called for validation and test\ndef evaluation_fun(model, dataloader, device, patch_size, folder, threshold=None):\n    model.eval()\n\n    all_image_scores, all_image_labels = [], []\n    good_samples_err = []\n    # AUPRO initialization\n    all_pixel_maps = []\n    all_gt_masks = []\n\n    with torch.no_grad():\n\n        for imgs, paths in dataloader:\n            imgs = imgs.to(device)\n            # no mask needed in validation and test\n            pred, _ = model(imgs)\n\n            # reconstruct the image\n            B = imgs.size(0)\n            H = W = hyperparameters[\"image_size\"]\n            recon = pred.view(B, H // patch_size, W // patch_size, patch_size, patch_size, hyperparameters['in_channels'])\n            recon = recon.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, 3, H, W)\n\n            # compute error map - check if 'good' in the path\n            err_maps = ((recon - imgs)**2).mean(dim=1)\n            image_scores_batch = torch.amax(err_maps, dim=[1, 2])\n            batch_scores = image_scores_batch.cpu().numpy()\n            all_image_scores.extend(batch_scores)\n            all_image_labels += [0 if \"good\" in p.lower() else 1 for p in paths]\n\n            # used in the threshold\n            for score, path in zip(batch_scores, paths):\n                if \"good\" in path.lower():\n                    good_samples_err.append(score)\n\n            # get ground truth masks\n            for i, path in enumerate(paths):\n                    all_pixel_maps.append(err_maps[i].cpu().numpy())\n                    # --- Modified section ---\n                    # Check if it's an anomaly image before trying to load a ground truth mask\n                    if \"good\" not in path.lower():\n                        gt_path = get_gt_mask_path(path, root_folder=folder)\n                        if os.path.exists(gt_path):\n                            gt_mask = Image.open(gt_path).convert(\"L\")\n                            # Resize mask to match image size for pixel-wise comparison\n                            gt_mask = np.array(gt_mask.resize((W, H), Image.NEAREST)) / 255.0\n                            all_gt_masks.append(gt_mask)\n                        else:\n                             # If an anomaly image has no GT mask, still add a zero mask for dimension consistency,\n                             # but this case should ideally not happen for anomalous samples in MVTec AD.\n                             # Print a warning or handle this as an error if necessary.\n                             print(f\"Warning: Ground truth mask not found for anomaly image: {path}\")\n                             gt_mask = np.zeros((H, W))\n                             all_gt_masks.append(gt_mask)\n                    else:\n                        # 'good' image case - no ground truth mask available, add a zero mask\n                        gt_mask = np.zeros((H,W))\n                        all_gt_masks.append(gt_mask)\n                    # --- End of modified section ---\n\n\n        # check if have good samples to evaluate best_threshold\n        if len(good_samples_err) == 0:\n            raise ValueError(\"No 'good' samples found in validation set\\n\")\n\n        all_image_scores = np.array(all_image_scores)\n        all_image_labels = np.array(all_image_labels)\n\n        # Validation case: calculate the threshold\n        if threshold is None:\n            best_threshold = np.percentile(good_samples_err, 95) #np.mean(good_samples_err) + 3 * np.std(good_samples_err)\n            best_class_threshold[c_name] = best_threshold\n            print(f\"Updated Class {c_name} threshold: {best_threshold:.3f}\")\n        # Test case: use the threshold passed as argument: no cheating allowed!\n        elif threshold is not None:\n            best_threshold = threshold\n        \n        # AUC and F1 (image wise)\n        auc = roc_auc_score(all_image_labels,  all_image_scores)\n        if best_threshold is not None:\n             preds = (all_image_scores >= best_threshold).astype(int)\n             f1  = f1_score(all_image_labels, preds)\n        else:\n             # If no threshold is available (e.g., no good samples), set F1 to 0\n             f1 = 0.0\n             print(\"F1 score not calculated due to missing threshold.\")\n\n\n        # AUPRO\n        aupro_scores = []\n        # Iterate through pixel maps and ground truth masks simultaneously\n        for pred_map, gtruth_mask in zip(all_pixel_maps, all_gt_masks):\n            # Only calculate AUPRO for samples with actual anomaly masks (sum > 0)\n            if gtruth_mask.sum() > 0:\n                 # Appiattisci la heatmap e la maschera GT\n                 scores = pred_map.flatten()\n                 masks = gtruth_mask.flatten()\n\n                 # Calcola AUPRO usando roc_auc_score\n                 # Ensure there are at least two unique classes (0 and 1) in the mask\n                 if len(np.unique(masks)) > 1:\n                     aupro = roc_auc_score(masks, scores)\n                     aupro_scores.append(aupro)\n                 else:\n                     # This case should only happen if a mask file exists but is all 0s or all 1s for an anomaly image\n                     print(f\"Warning: Ground truth mask for AUPRO calculation has only one unique value for a potential anomaly sample.\")\n\n\n        if len(aupro_scores) > 0:\n            aupro_mean = np.mean(aupro_scores)\n        else:\n            # If no anomaly samples with valid GT masks were found after filtering\n            aupro_mean = 0.0\n            print(\"Warning: No valid anomaly samples with corresponding GT masks found for AUPRO calculation after filtering.\")\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    # ROC graph - Plotting is done regardless of whether threshold/F1 was calculated\n    fpr, tpr, _ = roc_curve(all_image_labels, all_image_scores)\n    ax[0].plot(fpr, tpr, label=f\"AUC={auc:.3f}\",color='#822433')\n    ax[0].plot([0,1],[0,1],'k--')\n    ax[0].set_xlabel(\"False Positive Rate\")\n    ax[0].set_ylabel(\"True Positive Rate\")\n    ax[0].set_title(\"ROC Curve\")\n    ax[0].legend()\n    ax[0].grid(True, linestyle='--', alpha=0.6)\n    \n    # Plot histogram about error distribution - Plotting is done regardless of threshold\n    ax[1].hist(all_image_scores[all_image_labels==0], bins=30, alpha=0.9, label='good', color='#006778')\n    ax[1].hist(all_image_scores[all_image_labels==1], bins=30, alpha=0.8, label='anomaly', color='#822433')\n    ax[1].axvline(best_threshold, color='k', linestyle='--', label=f'thr={best_threshold:.3f}')\n    #else:\n    # If thresholding failed, maybe indicate this on the plot or in the title\n    #     plt.title(\"Error Distribution (Threshold not available)\");\n    ax[1].set_xlabel(\"error value\")\n    ax[1].legend()\n    ax[1].set_title(\"Error Distribution\")\n    ax[1].grid(True, linestyle='--', alpha=0.6)\n\n    plt.tight_layout()\n    plt.show()\n    # Return best_threshold as the fourth value in the tuple, even if it's a fallback or None\n    return auc, f1, aupro_mean, best_threshold","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fine-tuning loop with fixed threshold from train set\nEPOCHS_FT = 40\nLEARNING_RATE_FT = 5e-5\nPATIENCE_FT = 20\nEVAL_STEP = 5\nWEIGHT_DECAY = 1e-2\n\n# class threshold dictionary\nbest_class_threshold = {}\n\nfor c_name, dt_loader in dataloaders.items():\n    print(f\"\\nRunning FINE-TUNING on class: {c_name} \")\n\n    # lists for storing and then plot auc and aupro metrics\n    auc_story = []\n    aupro_story = []\n    epochs_recorded = [] # Store the epoch number when metrics are recorded\n\n    # load the pretrained model\n    model = MAE(**hyperparameters).to(device)\n    model.load_state_dict(torch.load(f\"/kaggle/working/{c_name}_pretrained_model.pth\"))\n    train_loader = dt_loader[\"train\"]\n    val_loader   = dt_loader[\"val\"]\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE_FT, weight_decay=WEIGHT_DECAY)\n    criterion = pixelwise_loss_mean\n    best_auc = 0.0\n    early_stop_count = 0\n\n    for epoch in range(1, EPOCHS_FT+1):\n        model.train()\n        ft_loss = train_one_ep(model, train_loader, optimizer, criterion, c_name, device)\n        print(f\"CLASS: {c_name} | EPOCH: {epoch}/{EPOCHS_FT} | LOSS: {ft_loss:.4f}\")\n\n        if epoch % EVAL_STEP == 0:\n            # validation\n            val_auc, val_f1, val_aupro, val_threshold = evaluation_fun(model, val_loader, device,\n                                                   patch_size=hyperparameters['patch_size'], folder=\"./mvtec_ad_validation\")\n            # store auc and aupro values\n            auc_story.append(val_auc)\n            aupro_story.append(val_aupro if val_aupro is not None else 0)\n            epochs_recorded.append(epoch) # Store the epoch number\n\n            print(f\"\\tEPOCH {epoch}/{EPOCHS_FT} Validation metrics: \")\n            print(f\"Val AUC: {val_auc:.3f} | Val F1: {val_f1:.3f} | Val AUPRO: {val_aupro:.3f}\")\n\n            # update threshold dictionary\n            best_class_threshold[c_name] = val_threshold\n\n            # early stopping\n            if val_auc > best_auc:\n                best_auc = val_auc\n                early_stop_count = 0\n                torch.save(model.state_dict(), f\"/kaggle/working/{c_name}_best_model_finetuned.pth\")\n                print(f\" New best finetuned model saved for class: {c_name}\")\n            else:\n                early_stop_count += 1\n                if early_stop_count >= PATIENCE_FT:\n                    print(\"Early stopping triggered.\")\n                    break\n\n    print(f\"-- Fine-tuning completed for class: {c_name} --\\n\")\n\n    # Use the recorded epochs for plotting\n    plot_metric_curve(epochs=epochs_recorded,\n                      values_dict={\"AUC\": auc_story,\"AUPRO\": aupro_story},\n                      title=f\"Validation Metrics - Class: {c_name}\",\n                      save_path=f\"/kaggle/working/visuals/{c_name}_metrics_plot.png\")\n\nprint(\"\\n---- Fine-tuning complete for all classes ----\\n\")\nprint(\"Best thresholds per class founded:\")\nfor c, th in best_class_threshold.items():\n    print(f\"{c}: {th:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation\n\nEvaluate the model on the test with F1 score, AUC and AUPRO\n","metadata":{}},{"cell_type":"code","source":"# Test\nprint(f\"\\n---- Test section ----\\n\")\n\nfor c_name, dt_loader in dataloaders.items():\n    print(f\"\\nTesting class: {c_name} \\n\")\n    \n    final_model = MAE(**hyperparameters).to(device)\n    path_saved_model = f\"/kaggle/working/{c_name}_best_model_finetuned.pth\"\n    model.load_state_dict(torch.load(path_saved_model))\n\n    test_loader = dt_loader[\"test\"]\n    # get the class threshold from the thresholds dictionary\n    class_threshold = best_class_threshold.get(c_name)\n    test_auc, test_f1, test_aupro, _ = evaluation_fun(model=final_model,dataloader=test_loader,device=device,\n                                                      patch_size=hyperparameters['patch_size'], folder=\"./mvtec_ad_test\", threshold=class_threshold)\n    \n    print(f\"\\nFinal results summary per class: {c_name}\")\n    print(f\"Threshold: {class_threshold:.3f}\")\n    print(f\"Test AUC: {test_auc:.3f}\")\n    print(f\"Test F1: {test_f1:.3f}\")\n    print(f\"Test AUPRO: {test_aupro:.3f}\\n\")\n    print(\"--------------------------------------\")    \n\nprint(f\"\\n---- THIS IS THE END of the Test section ----\\n\")    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **EXTRA!**","metadata":{}},{"cell_type":"code","source":"# Change class_name to visualize different class samples \nvisualize_with_gt(model, dataloaders, class_name=\"hazelnut\", device=device, \n                  hyperparams=hyperparameters,root_folder='./mvtec_ad_validation', num_images=10, save_dir='./pictures/finetuned')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# potremmo implementare una parte di training and validation loss plotting come fanno anche loro\nprint(\"Dataloaders keys:\", dataloaders.keys())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}